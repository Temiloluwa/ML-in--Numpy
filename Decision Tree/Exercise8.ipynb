{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temiloluwa Adeoti, 303008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score by Sklearn for Iris Dataset 0.9555555555555556\n",
      "Accuracy score by Sklearn for Breast Cancer Dataset 0.9181286549707602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "iris_data = load_iris(return_X_y=False)\n",
    "breast_cancer_data = load_breast_cancer(return_X_y=False)\n",
    "\n",
    "X_iris,y_iris = iris_data['data'],iris_data['target']\n",
    "X_breast,y_breast = breast_cancer_data['data'],breast_cancer_data['target']\n",
    "\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "            X_iris, y_iris, test_size=0.3, random_state=2019)\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "            X_breast, y_breast, test_size=0.3, random_state=2019)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train_i,  y_train_i)\n",
    "print(f\"Accuracy score by Sklearn for Iris Dataset {clf.score(X_test_i,y_test_i)}\")\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train_b,  y_train_b)\n",
    "print(f\"Accuracy score by Sklearn for Breast Cancer Dataset {clf.score(X_test_b,y_test_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_tree:\n",
    "    def __init__(self):\n",
    "        self.ypred = None\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        self.tree = Tree()(X,Y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        y_pred = np.empty(len(X))\n",
    "        for row in range(len(X)):\n",
    "            query = X[row]\n",
    "            pred  = self.tranverse_tree(query)\n",
    "            y_pred[row] = pred\n",
    "        self.ypred = y_pred\n",
    "        print(f\"prediction is {y_pred}\")\n",
    "        return self\n",
    "    \n",
    "    def score(self,Y):\n",
    "        return np.mean(self.ypred == Y)\n",
    "    \n",
    "    def tranverse_tree(self,query):\n",
    "        tree_dict = self.tree.__dict__\n",
    "        while tree_dict and not tree_dict['is_leaf']:\n",
    "            deciding_feature = tree_dict['feature']\n",
    "            split_rule       = tree_dict['split_rule']\n",
    "            if query[deciding_feature] < split_rule:\n",
    "                tree_dict = tree_dict['left_child'].__dict__\n",
    "            else:\n",
    "                tree_dict = tree_dict['right_child'].__dict__\n",
    "        return tree_dict['leaf']\n",
    "\n",
    "    \n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.is_leaf     = False\n",
    "        self.left_child  = None\n",
    "        self.right_child = None\n",
    "        self.feature     = None\n",
    "        self.split_rule  = None\n",
    "        \n",
    "    def __call__(self,X,Y):\n",
    "        if not self.is_leaf:\n",
    "            return self.split(X,Y)\n",
    "    \n",
    "    def split(self,X,Y):\n",
    "        n_samples,n_features = X.shape\n",
    "        positive_class       = Y[0]\n",
    "        lowest_gini          = math.inf\n",
    "        best_split           = []\n",
    "        \n",
    "        if self.stop_criterion(X,Y):\n",
    "            self.leaf = Y[0]\n",
    "            self.is_leaf = True\n",
    "            return self\n",
    "        else:\n",
    "            for i in range(n_features):\n",
    "                feat          = X[:,i]\n",
    "                unique_values = np.unique(feat)\n",
    "                mids          = self.midpoints(unique_values)\n",
    "                 \n",
    "                for val in mids:\n",
    "                    split           = self._make_rule(X,i,val)\n",
    "                    below_criterion = Y[split]\n",
    "                    above_criterion = Y[~split]\n",
    "\n",
    "                    below_posi_class_prob = np.sum(below_criterion == positive_class)/len(below_criterion)\n",
    "                    below_neg_class_prob  = 1 -  below_posi_class_prob\n",
    "\n",
    "                    above_posi_class_prob = np.sum(above_criterion == positive_class)/len(above_criterion)\n",
    "                    above_posi_class_prob  = 1 -  above_posi_class_prob\n",
    "\n",
    "                    below_gini = 1 -  below_posi_class_prob**2 - below_neg_class_prob**2\n",
    "                    above_gini = 1 -  above_posi_class_prob**2 - above_posi_class_prob**2\n",
    "\n",
    "                    weigh_below  = len(below_criterion)/len(Y)\n",
    "                    weigh_above  = len(above_criterion)/len(Y)\n",
    "\n",
    "                    weighted_gini = weigh_below*below_gini + weigh_above * above_gini\n",
    "\n",
    "                    if weighted_gini < lowest_gini:\n",
    "                        lowest_gini = weighted_gini\n",
    "                        best_split  = split\n",
    "                        self.feature     = i\n",
    "                        self.split_rule  = val\n",
    "        \n",
    "        if len(X[best_split]) == len(Y[best_split]):           \n",
    "            self.left_child  = Tree()(X[best_split], Y[best_split])\n",
    "            self.right_child = Tree()(X[~best_split], Y[~best_split])\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def midpoints(x):\n",
    "        #returns a array with midvalues of 2 consequtive array elements\n",
    "        mid = np.empty(len(x)-1)\n",
    "        for i in range(len(x)-1):\n",
    "            mid[i] = (x[i] + x[i+1])/2\n",
    "        return mid\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def stop_criterion(X,Y):\n",
    "        #there is only 1 sample left\n",
    "        if len(X) == 1 or len(Y) == 1:\n",
    "            return True\n",
    "        # data belongs to the same class\n",
    "        if np.prod(Y == Y[0]):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_rule (X,idx , val):\n",
    "        # return the splitting rule ( univariate splits for numerical data )\n",
    "        return X[:, idx] < val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is [0. 0. 2. 1. 2. 0. 2. 0. 2. 2. 2. 2. 2. 1. 0. 1. 2. 2. 0. 2. 0. 2. 0. 2.\n",
      " 0. 0. 1. 2. 1. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 2. 0. 1. 0. 2. 2.]\n",
      "Accuracy score by Self Implementation for Iris Dataset 0.8666666666666667\n",
      "\n",
      "prediction is [1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1.]\n",
      "Accuracy score by Self Implementation for Breast Cancer Dataset 0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "cf = Decision_tree()\n",
    "cf = cf.fit(X_train_i,y_train_i)\n",
    "cf.predict(X_test_i)\n",
    "print(f\"Accuracy score by Self Implementation for Iris Dataset {cf.score(y_test_i)}\\n\")\n",
    "\n",
    "cf = Decision_tree()\n",
    "cf = cf.fit(X_train_b,y_train_b)\n",
    "cf.predict(X_test_b)\n",
    "print(f\"Accuracy score by Self Implementation for Breast Cancer Dataset {cf.score(y_test_b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Node of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left_child': <__main__.Tree at 0x1890b44fa48>,\n",
       " 'right_child': <__main__.Tree at 0x1890b448388>,\n",
       " 'feature': 9,\n",
       " 'split_rule': 0.055349999999999996}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.tree.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision\n",
    "The Scores produced by sklearn was better\n",
    "- Iris Dataset, Sklearn =  0.9555555555555556, Self = 0.8666666666666667\n",
    "- Breast Cancer Dataset = 0.9181286549707602, Self = 0.8421052631578947"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
